[{"title":"python json 库性能对比","path":"/2024/10/07/python-json-库性能对比/","content":"主要挑选了三个库进行对比: json, orjson, simdjson orjson 开源社区：https://github.com/ijl/orjson orjson 是一个快速、正确的 Python JSON 库。它 被评为最快的 JSON Python 库，并且比标准 json 库或其他第三方库更正确。它本机序列化 dataclass、 datetime、 numpy和 UUID实例。 与其他Python JSON库相比，它的特点和缺点： 序列化dataclass实例的速度是其他库的 40-50 倍 将datetime、date、 和time实例序列化为 RFC 3339 格式，例如“1970-01-01T00:00:00+00:00” 序列化numpy.ndarray实例的速度是其他库的 4-12 倍，内存使用量是其他库的 0.3 倍 漂亮的打印速度是标准库的 10 到 20 倍 序列化为bytes而不是str，即不是直接替换 序列化时str不将 unicode 转义为 ASCII，例如“好”而不是“\\u597d” 序列化float速度是其他库的 10 倍，反序列化速度是其他库的两倍 原生序列化str、int、list和 的子类dict，需要default指定如何序列化其他类 default使用钩子序列化任意类型 具有严格的UTF-8一致性，比标准库更正确 具有严格的 JSON 一致性，不支持 Nan&#x2F;Infinity&#x2F;-Infinity 具有对 53 位整数严格遵循 JSON 的选项，默认支持 64 位 不提供读取&#x2F;写入类文件对象的load()功能dump() simdjson 开源社区：https://github.com/simdjson/simdjson JSON 在互联网上随处可见。服务器花费“大量”时间来解析它。我们需要一种新的方法。simdjson 库使用常用的 SIMD 指令和微并行算法，解析 JSON 的速度比 RapidJSON 快 4 倍，比现代 C++ 的 JSON 快 25 倍。 快速：比常用的生产级 JSON 解析器快 4 倍以上。 破纪录的功能：以 6 GB&#x2F;s 的速度压缩 JSON，以 13 GB&#x2F;s 的速度验证 UTF-8，以 3.5 GB&#x2F;s 的速度验证 NDJSON。 简单：一流、易于使用且详细记录的 API。 严格：完整的 JSON 和 UTF-8 验证，无损解析。性能毫不妥协。 自动：在运行时选择适合 CPU 的解析器。无需配置。 可靠：从内存分配到错误处理，simdjson 的设计避免了意外。 测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import jsonimport orjsonimport timeimport simdjsonimport matplotlib.pyplot as plttest_data = &#123; &quot;key1&quot;: &quot;value1&quot;,省略.. &quot;key100&quot;: &quot;value100&quot;&#125;test_data_str = json.dumps(test_data)def average_every_n(data, n): averaged_data = [] for i in range(0, len(data), n): chunk = data[i:i + n] averaged_data.append(sum(chunk) / len(chunk)) return averaged_datadef json_t(): times = [] for i in range(1, 10001): start_time = time.time() json.dumps(test_data) end_time = time.time() times.append(end_time - start_time) if i % 1000 == 0: averaged_data = average_every_n(times, 1000) yield i // 1000, averaged_data[-1]def orjson_t(): times = [] for i in range(1, 10001): start_time = time.time() orjson.dumps(test_data).decode(&#x27;utf-8&#x27;) end_time = time.time() times.append(end_time - start_time) if i % 1000 == 0: averaged_data = average_every_n(times, 1000) yield i // 1000, averaged_data[-1]def simdjson_t(): times = [] for i in range(1, 10001): start_time = time.time() simdjson.dumps(test_data) end_time = time.time() times.append(end_time - start_time) if i % 1000 == 0: averaged_data = average_every_n(times, 1000) yield i // 1000, averaged_data[-1]if __name__ == &#x27;__main__&#x27;: json_data = list(json_t()) orjson_data = list(orjson_t()) simdjson_data = list(simdjson_t()) # 绘制对比图 plt.plot(*zip(*json_data), label=&#x27;json&#x27;) plt.plot(*zip(*orjson_data), label=&#x27;orjson&#x27;) plt.plot(*zip(*simdjson_data), label=&#x27;simdjson&#x27;) plt.xlabel(&#x27;Iteration (x1000)&#x27;) plt.ylabel(&#x27;Average Time (seconds)&#x27;) plt.legend() plt.title(&#x27;JSON Serialization Performance Comparison&#x27;) plt.show() 测试结果注意：由于 orjson 是将对象序列化为bytes而不是str，即不是直接替换，所以加上 decode(&#39;utf-8&#39;) 将其转换成字符串 json dumps 的性能对比 json loads 的新能对比 可见 orjson 的性能是远超 python 的内置库和 simdjson 库的","tags":["python"]},{"title":"uvicorn-源码浅读","path":"/2024/10/07/uvicorn-源码浅读/","content":"前记Uvicorn是一个基于uvloop和httptools的ASGI服务器, 性能比较强劲， 通过它可以与使用ASGI规范的Python应用程序进行交互。ASGI与WSGI很像， 只不过ASGI原生支持HTTP2.0和WebSocket， 同时更多的是支持Python的Asyncio生态的WEB应用程序。通过了解Uvicron，能知道一个稳定的Web服务器的工作方式以及能更好的去了解其他基于ASGI的WEB应用程序 知识点 1uvloop 是一个用 Cython 编写的 asyncio 事件循环的快速替代品，它基于 libuv，这是一个高性能的跨平台异步 I&#x2F;O 库。uvloop 被设计为可以无缝替换 Python 标准库 asyncio 中的默认事件循环，从而提高异步编程的性能。它至少比 Node.js、gevent 以及其他 Python 异步框架要快两倍，其性能接近 Go 程序。 开源仓库：https://github.com/MagicStack/uvloop 使用方法 12345678pip install uvloopimport asyncioimport uvloopasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())# 编写asyncio的代码，与之前写的代码一致。# 内部的事件循环自动化会变为uvloopasyncio.run(...) 在 uvicorn 中的使用 1234567891011121314151617181920uvicorn/loops/auto.pydef auto_loop_setup(use_subprocess: bool = False) -&gt; None: try: import uvloop # noqa except ImportError: # pragma: no cover from uvicorn.loops.asyncio import asyncio_setup as loop_setup loop_setup(use_subprocess=use_subprocess) else: # pragma: no cover from uvicorn.loops.uvloop import uvloop_setup uvloop_setup(use_subprocess=use_subprocess)uvicorn/loops/uvloop.pyimport asyncioimport uvloopdef uvloop_setup(use_subprocess: bool = False) -&gt; None: asyncio.set_event_loop_policy(uvloop.EventLoopPolicy()) 缺点 目前不支持 windows 知识点 2ASGI（Asynchronous Server Gateway Interface）是一个用于网络应用的异步Python框架接口，用于处理HTTP和WebSocket请求。ASGI是同步WSGI（Web Server Gateway Interface）接口的异步继任者，它允许使用Python编写的异步框架来构建高性能的Web应用, 为什么使用 ASGI Most well established Python Web frameworks started out as WSGI-based frameworks. 大多数成熟的 Python Web 框架最初都是基于 WSGI 的框架。 WSGI applications are a single, synchronous callable that takes a request and returns a response. This doesn’t allow for long-lived connections, like you get with long-poll HTTP or WebSocket connections, which WSGI doesn’t support well. WSGI 应用程序是一个单一的同步可调用对象，它接受一个请求并返回一个响应。这不允许长时间保持连接，例如你在长轮询 HTTP 或 WebSocket 连接中所获得的那种，而 WSGI 对这些并不支持得很好。 Having an async concurrency model also allows for options such as lightweight background tasks, and can be less of a limiting factor for endpoints that have long periods being blocked on network I&#x2F;O such as dealing with slow HTTP requests. 采用异步并发模型还可以实现轻量级的后台任务，可以更少的被那些长时间被网络I&#x2F;O（如处理缓慢的HTTP请求）阻塞的端点所限制。 文档链接: https://www.uvicorn.org/#the-asgi-interface 最简单的 ASGI 应用 1234567891011121314async def app(scope, receive, send): assert scope[&#x27;type&#x27;] == &#x27;http&#x27; await send(&#123; &#x27;type&#x27;: &#x27;http.response.start&#x27;, &#x27;status&#x27;: 200, &#x27;headers&#x27;: [ [b&#x27;content-type&#x27;, b&#x27;text/plain&#x27;], ], &#125;) await send(&#123; &#x27;type&#x27;: &#x27;http.response.body&#x27;, &#x27;body&#x27;: b&#x27;Hello, world!&#x27;, &#125;) scope http 的内容： 1234567891011121314scope = &#123; &quot;type&quot;: &quot;http&quot;, &quot;http_version&quot;: &quot;1.1&quot;, &quot;method&quot;: &quot;GET&quot;, &quot;scheme&quot;: &quot;https&quot;, &quot;path&quot;: &quot;/&quot;, &quot;query_string&quot;: b&quot;search=red+blue&amp;maximum_price=20&quot;, &quot;headers&quot;: [ (b&quot;host&quot;, b&quot;www.example.org&quot;), (b&quot;accept&quot;, b&quot;application/json&quot;) ], &quot;client&quot;: (&quot;134.56.78.4&quot;, 1453), &quot;server&quot;: (&quot;www.example.org&quot;, 443)&#125; 与 wsgi 的 environ 字典非常相似 12345678910111213environ = &#123; &quot;REQUEST_METHOD&quot;: &quot;GET&quot;, &quot;SCRIPT_NAME&quot;: &quot;&quot;, &quot;PATH_INFO&quot;: &quot;/&quot;, &quot;QUERY_STRING&quot;: &quot;search=red+blue&amp;maximum_price=20&quot;, &quot;SERVER_NAME&quot;: &quot;www.example.org&quot;, &quot;SERVER_PORT&quot;: 443, &quot;REMOTE_HOST&quot;: &quot;134.56.78.4&quot;, &quot;REMOTE_PORT&quot;: 1453, &quot;SERVER_PROTOCOL&quot;: &quot;HTTP/1.1&quot;, &quot;HTTP_HOST&quot;: &quot;www.example.org&quot;, &quot;HTTP_ACCEPT&quot;: &quot;application/json&quot;,&#125; 实际上官访文档也解释了两者之间的兼容性：https://asgi.readthedocs.io/en/latest/specs/www.html#wsgi-compatibility 源码结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253├─lifespan│ │ off.py│ │ on.py│ │ __init__.py│├─loops│ │ asyncio.py│ │ auto.py│ │ uvloop.py│ │ __init__.py│├─middleware│ │ asgi2.py│ │ message_logger.py│ │ proxy_headers.py│ │ wsgi.py│ │ __init__.py│├─protocols│ │ utils.py│ │ __init__.py│ ││ ├─http│ │ │ auto.py│ │ │ flow_control.py│ │ │ h11_impl.py│ │ │ httptools_impl.py│ │ │ __init__.py│ ││ ├─websockets│ │ │ auto.py│ │ │ websockets_impl.py│ │ │ wsproto_impl.py│ │ │ __init__.py│├─supervisors│ │ basereload.py│ │ multiprocess.py│ │ statreload.py│ │ watchfilesreload.py│ │ watchgodreload.py│ │ __init__.py││ config.py│ importer.py│ logging.py│ main.py│ server.py│ workers.py│ _subprocess.py│ _types.py│ __init__.py│ __main__.py lifespan 告诉基于 ASGI 的应用程序uvicorn即将启动和停止的时间， uvicorn 在启动的时候会初始化，然后发送初始化协议并等待ASGI应用程序返回， 如果ASGI应用程序返回 complele 则 uvicorn 会继续运行， 返回 failed 则报错退出 我们先看 lifespan 这个目录有什么 123456789101112131415161718192021222324252627282930313233343536373839class LifespanOn: def __init__(self, config: Config) -&gt; None: self.logger = logging.getLogger(&quot;uvicorn.error&quot;) self.startup_event = asyncio.Event() self.shutdown_event = asyncio.Event() self.receive_queue: Queue[LifespanReceiveMessage] = asyncio.Queue() ... async def startup(self) -&gt; None: self.logger.info(&quot;Waiting for application startup.&quot;) # 获取事件循环 loop = asyncio.get_event_loop() # 这里只是创建了任务，让其在事件循环中等待执行，创建变量原因是添加 hard reference, 避免被 python 自动 gc main_lifespan_task = loop.create_task(self.main()) # noqa: F841 # Keep a hard reference to prevent garbage collection # See https://github.com/encode/uvicorn/pull/972 startup_event: LifespanStartupEvent = &#123;&quot;type&quot;: &quot;lifespan.startup&quot;&#125; # 发送启动事件到队列 await self.receive_queue.put(startup_event) await self.startup_event.wait() if self.startup_failed or (self.error_occured and self.config.lifespan == &quot;on&quot;): self.logger.error(&quot;Application startup failed. Exiting.&quot;) self.should_exit = True else: self.logger.info(&quot;Application startup complete.&quot;) async def main(self) -&gt; None: try: app = self.config.loaded_app scope: LifespanScope = &#123; &quot;type&quot;: &quot;lifespan&quot;, &quot;asgi&quot;: &#123;&quot;version&quot;: self.config.asgi_version, &quot;spec_version&quot;: &quot;2.0&quot;&#125;, &quot;state&quot;: self.state, &#125; await app(scope, self.receive, self.send) except BaseException as exc: ... asyncio.Event() 对象的使用 123456789101112131415161718192021222324252627282930313233import asyncioasync def waiter(event): print(&quot;Waiter is waiting for the event to be set.&quot;) await event.wait() print(&quot;Waiter has been awakened.&quot;)async def setter(event): print(&quot;Setter is going to set the event.&quot;) await asyncio.sleep(1) # 模拟一些异步操作 event.set() print(&quot;Event has been set.&quot;)async def main(): event = asyncio.Event() waiter_task = asyncio.create_task(waiter(event)) setter_task = asyncio.create_task(setter(event)) # 等待两个任务都完成 await waiter_task await setter_taskasyncio.run(main())# 输出Waiter is waiting for the event to be set.Setter is going to set the event.Event has been set.Waiter has been awakened. LifespanOff 比较简单 12345678910class LifespanOff: def __init__(self, config: Config) -&gt; None: self.should_exit = False self.state: dict[str, Any] = &#123;&#125; async def startup(self) -&gt; None: pass async def shutdown(self) -&gt; None: pass loops自动加载事件循环， 优先加载 uvloop 参考上文 知识点 1 middleware 里面是一些简单通用的中间件 PS: 这里的中间件和 fastapi 中的中间件是不同的，fastapi 的中间件是一个 ASGI 应用中上下文的切割，这里的中间件就是一个 ASGI 应用 wsgi.py 将 ASGI 服务转成 WSGI 服务 message_logger.py 传递消息的中间件 proxy_headers.py 一个应用程序通过代理（如Nginx、Apache等）与客户端连接时，客户端的请求会先经过代理服务器，然后再转发给应用程序。代理服务器在转发时，会添加一些特殊的HTTP头部，比如： X-Forwarded-For：记录原始客户端的IP地址。 X-Forwarded-Proto：记录客户端使用的协议（HTTP或HTTPS）。 然而，当应用程序收到请求时，默认情况下，它会认为请求是从代理服务器而不是从原始客户端发来的。所以，客户端的IP和协议信息可能会被代理的IP和协议覆盖。 这个中间件的作用就是让应用程序使用代理服务器传递过来的这些头部信息，从而获取到原始客户端的IP地址和协议，而不是代理服务器的IP和协议。这样，应用程序可以知道真正发起请求的客户端是谁，以及他们使用的是HTTP还是HTTPS。 protocols 里面存放着读取连接数据和解析消息体的协议， 如 HTTP 和 WebSockets, 可以把他认为是一个序列化器。 基础协议uvicorn 封装的对象继承于 asyncio.Protocol, 它是针对TCP协议的封装，包含以下六个方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243class BaseProtocol: def connection_made(self, transport): &quot;&quot;&quot; 在建立连接时调用. 参数是表示管道连接的transport, 此时得到的transport需要设置为该类的transport， 方便后续connection_lost控制关闭管道. &quot;&quot;&quot; def connection_lost(self, exc): &quot;&quot;&quot; 当连接丢失或关闭时调用, 根据exc判断是否要关闭trnasport. 参数是一个异常对象或None(后者表示接收到常规EOF或中止或关闭连接). &quot;&quot;&quot; def pause_writing(self): &quot;&quot;&quot; 当transport缓冲区超过高水位(high-water mark)时调用, 此时应该能控制外部不再写入数据（通常是一个asyncio.Future）, 同时应该通过transport.pause_reading来停止获取数据， 之后TCP就会通过拥塞机制使得客户端减缓发送数据的速度。 &quot;&quot;&quot; def resume_writing(self): &quot;&quot;&quot; 当transport缓冲区排放低于低水位线(low-water mark)时调用. 此时要释放标志， 使得外部可以继续写入数据， 同时通过transport.resume_reading来恢复获取数据， 之后TCP就会通过拥塞机制知道服务端的处理能力上来了， 使客户端加快发送速度。 &quot;&quot;&quot;class Protocol(BaseProtocol): def data_received(self, data): &quot;&quot;&quot; 通过该方法可以获取到客户端传输过来的数据 &quot;&quot;&quot; def eof_received(self): &quot;&quot;&quot; 当另一端调用write_eof()或等效函数时调用. 如果返回一个假值(包括None)，则传输将关闭自身。 如果它返回true值，则关闭传输取决于协议. &quot;&quot;&quot; 通过这些我们还不知道 uvicorn 做了哪些修改来达到跟应用程序进行通信，所以我们接着看一下 uvicorn 的具体实现 http里面有两种实现：httptools 和 h11_impl httptools 是一个用 C 语言编写的高性能 HTTP 库，它提供了一个快速的 HTTP 解析器和生成器。由于它是用 C 编写的，因此可以提供比纯 Python 库更高的性能。httptools 通常用于构建高性能的 Web 服务器和客户端，特别是在需要处理大量并发连接时。 h11 是一个纯 Python 编写的 HTTP&#x2F;1.1 协议的实现，它提供了一个简单、明确的 API 来处理 HTTP 消息。h11 的设计目标是易于理解和使用，而不是追求最高性能。 httptools 的内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class ServerState: &quot;&quot;&quot; Shared servers state that is available between all protocol instances. &quot;&quot;&quot; def __init__(self) -&gt; None: self.total_requests = 0 self.connections: set[Protocols] = set() self.tasks: set[asyncio.Task[None]] = set() self.default_headers: list[tuple[bytes, bytes]] = []class HttpToolsProtocol(asyncio.Protocol): def __init__( self, config: Config, server_state: ServerState, app_state: dict[str, Any], _loop: asyncio.AbstractEventLoop | None = None, ) -&gt; None: if not config.loaded: config.load() self.config = config self.app = config.loaded_app self.loop = _loop or asyncio.get_event_loop() self.logger = logging.getLogger(&quot;uvicorn.error&quot;) self.access_logger = logging.getLogger(&quot;uvicorn.access&quot;) # 传入了 self self.parser = httptools.HttpRequestParser(self) # server_state 可以理解为一个容器 self.server_state = server_state self.connections = server_state.connections self.tasks = server_state.tasks # Per-connection state self.transport: asyncio.Transport = None # type: ignore[assignment] self.flow: FlowControl = None # type: ignore[assignment] # 请求处理流程的双端队列 self.pipeline: deque[tuple[RequestResponseCycle, ASGI3Application]] = deque() def connection_made( # type: ignore[override] self, transport: asyncio.Transport ) -&gt; None: # 添加实例本身到集合， 代表当前还有连接在处理 self.connections.add(self) self.transport = transport # 初始化流控制 self.flow = FlowControl(transport) # 简单的初始化实例trsnaport的相关编列 self.server = get_local_addr(transport) self.client = get_remote_addr(transport) self.scheme = &quot;https&quot; if is_ssl(transport) else &quot;http&quot; if self.logger.level &lt;= TRACE_LOG_LEVEL: prefix = &quot;%s:%d - &quot; % self.client if self.client else &quot;&quot; self.logger.log(TRACE_LOG_LEVEL, &quot;%sHTTP connection made&quot;, prefix) def connection_lost(self, exc: Exception | None) -&gt; None: # 从集合删除实例本身， 代表当前连接已经处理玩了， 不需要进入统计容器 self.connections.discard(self) if self.logger.level &lt;= TRACE_LOG_LEVEL: prefix = &quot;%s:%d - &quot; % self.client if self.client else &quot;&quot; self.logger.log(TRACE_LOG_LEVEL, &quot;%sHTTP connection lost&quot;, prefix) # 设置cycle， 告诉他连接已经断开 if self.cycle and not self.cycle.response_complete: self.cycle.disconnected = True if self.cycle is not None: self.cycle.message_event.set() if self.flow is not None: self.flow.resume_writing() if exc is None: self.transport.close() self._unset_keepalive_if_required() self.parser = None def _unset_keepalive_if_required(self): &quot;&quot;&quot;取消keep alive timeout的任务， 一般来说， 在发送数据后服务端会等待客户端发送数据， 如果超过多少秒没有发送数据则可以判断该客户端已经断开了， 服务端可以主动关闭连接 而uvicorn通过timeout_keep_alive_task来实现 &quot;&quot;&quot; if self.timeout_keep_alive_task is not None: self.timeout_keep_alive_task.cancel() self.timeout_keep_alive_task = None def data_received(self, data): self._unset_keepalive_if_required() try: # 接受字节数据， 并交由http解析器进行解析 self.parser.feed_data(data) except httptools.HttpParserError as exc: # 解析失败， 应该不是http协议的数据， 断开连接 msg = &quot;Invalid HTTP request received.&quot; self.logger.warning(msg, exc_info=exc) self.transport.close() except httptools.HttpParserUpgrade: # 已经超过了解析器能解析的协议版本， 应该交由更新的协议解析器处理 self.handle_upgrade() 该类中还有很多 on_xxx 的方法并没有被调用，是因为在初始化HTTP协议解析器的时候，uvicorn.protocol 把自己的实例传入了HTTP解析器中， 解析器会边接收数据边按照url, header, body来顺序解析， 并在执行每种数据解析后， 会通过回调告诉传入的实例， uvicorn 正是通过on_xxx 方法来监听这些回调并处理解析完的HTTP数据 可以看 httptools 的 parser 源码，是 CPython 写的: https://github.com/MagicStack/httptools/blob/master/httptools/parser/parser.pyx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133class HttpToolsProtocol(asyncio.Protocol): def on_message_begin(self) -&gt; None: # 接收到请求的第一次解析，主要作用是初始化 scope self.url = b&quot;&quot; self.expect_100_continue = False self.headers = [] self.scope = &#123; # type: ignore[typeddict-item] &quot;type&quot;: &quot;http&quot;, &quot;asgi&quot;: &#123;&quot;version&quot;: self.config.asgi_version, &quot;spec_version&quot;: &quot;2.4&quot;&#125;, &quot;http_version&quot;: &quot;1.1&quot;, &quot;server&quot;: self.server, &quot;client&quot;: self.client, &quot;scheme&quot;: self.scheme, # type: ignore[typeddict-item] &quot;root_path&quot;: self.root_path, &quot;headers&quot;: self.headers, &quot;state&quot;: self.app_state.copy(), &#125; # Parser callbacks def on_url(self, url: bytes) -&gt; None: self.url += url def on_header(self, name: bytes, value: bytes) -&gt; None: &quot;&quot;&quot;解析器在解析header时， 是按照header一行一行进行解析的， 所以每一行header都会调用一次on_header, 并把他们存在实例的headers中&quot;&quot;&quot; name = name.lower() if name == b&quot;expect&quot; and value.lower() == b&quot;100-continue&quot;: self.expect_100_continue = True self.headers.append((name, value)) def on_headers_complete(self) -&gt; None: http_version = self.parser.get_http_version() method = self.parser.get_method() self.scope[&quot;method&quot;] = method.decode(&quot;ascii&quot;) if http_version != &quot;1.1&quot;: self.scope[&quot;http_version&quot;] = http_version if self.parser.should_upgrade() and self._should_upgrade(): # 如果发现当前http版本更加高级（比如websocket）, 则不再处理, 在另外一个逻辑会转到websocket处理 return parsed_url = httptools.parse_url(self.url) raw_path = parsed_url.path path = raw_path.decode(&quot;ascii&quot;) if &quot;%&quot; in path: path = urllib.parse.unquote(path) full_path = self.root_path + path full_raw_path = self.root_path.encode(&quot;ascii&quot;) + raw_path self.scope[&quot;path&quot;] = full_path self.scope[&quot;raw_path&quot;] = full_raw_path self.scope[&quot;query_string&quot;] = parsed_url.query or b&quot;&quot; # Handle 503 responses when &#x27;limit_concurrency&#x27; is exceeded. if self.limit_concurrency is not None and ( len(self.connections) &gt;= self.limit_concurrency or len(self.tasks) &gt;= self.limit_concurrency ): # 当前并发数过高， 不再转发给后面的应用程序， 直接返回错误, 这里是一个具有ASGI标准函数签名的函数, 里面实现的功能是发送错误信息到socket app = service_unavailable message = &quot;Exceeded concurrency limit.&quot; self.logger.warning(message) else: app = self.app # cycle 相当于一个 request 的处理流程,普通的HTTP请求只对应一个cycle就可以了，这里是兼容Pipeline HTTP请求 existing_cycle = self.cycle self.cycle = RequestResponseCycle( scope=self.scope, transport=self.transport, flow=self.flow, logger=self.logger, access_logger=self.access_logger, access_log=self.access_log, default_headers=self.server_state.default_headers, message_event=asyncio.Event(), expect_100_continue=self.expect_100_continue, keep_alive=http_version != &quot;1.0&quot;, on_response=self.on_response_complete, ) if existing_cycle is None or existing_cycle.response_complete: # Standard case - start processing the request. # 如果上个请求已经处理完了， 则开始处理这个请求(通过run_asgi来运行) task = self.loop.create_task(self.cycle.run_asgi(app)) task.add_done_callback(self.tasks.discard) self.tasks.add(task) else: # 如果上个请求没有处理完(比如 body 没有接收完整)， 就先暂停读取数据， 并把该cycle放到pipeline暂存 # Pipelined HTTP requests need to be queued up. self.flow.pause_reading() self.pipeline.appendleft((self.cycle, app)) def on_body(self, body: bytes) -&gt; None: # 一个请求可能会有多次 on_body 调用，包括这里再往后就没有直接解析数据了，一般会发送到真正处理的应用程序（ASGI应用） if (self.parser.should_upgrade() and self._should_upgrade()) or self.cycle.response_complete: return # 将 body 累计到 cycle 中 self.cycle.body += body if len(self.cycle.body) &gt; HIGH_WATER_LIMIT: # 由于ASGI应用程序会根据调用者需要才来获取body(比如starlette的 await request.body())， 如果应用程序没有需要则会暂缓获取body数据 self.flow.pause_reading() # 告诉ASGI应用程序， body已经获取结束（通常在cycle的more_body为False的时候, 才会检查message_event） self.cycle.message_event.set() def on_message_complete(self) -&gt; None: if (self.parser.should_upgrade() and self._should_upgrade()) or self.cycle.response_complete: return # 表示body已经读取结束了 self.cycle.more_body = False self.cycle.message_event.set() def on_response_complete(self) -&gt; None: # Callback for pipelined HTTP requests to be started. self.server_state.total_requests += 1 if self.transport.is_closing(): return # 设置一个keep_alive的机制， 服务端返回响应后会设置一个倒计时future, 该future只有在上面data_received收到请求的时候才会取消 # 如果该future没有取消， 则会调用timeout_keep_alive_handler函数来关闭transport通道 self._unset_keepalive_if_required() # Unpause data reads if needed. self.flow.resume_reading() # Unblock any pipelined events. If there are none, arm the # Keep-Alive timeout instead. if self.pipeline: # 如果是pipeline请求， 则开始处理刚才暂存的cycle cycle, app = self.pipeline.pop() task = self.loop.create_task(cycle.run_asgi(app)) task.add_done_callback(self.tasks.discard) self.tasks.add(task) else: self.timeout_keep_alive_task = self.loop.call_later( self.timeout_keep_alive, self.timeout_keep_alive_handler ) PS: 什么是 pipeline 请求 在ASGI的上下文中，一个请求可能不会立即完成，而是会通过多个事件来分阶段处理。例如，一个HTTP请求可能首先接收到请求头，然后是请求体的一部分，然后是更多的请求体，直到所有的请求体都被接收。这个过程可以被看作是一个”pipeline”，其中请求数据在被完全接收和处理之前，会通过不同的阶段流动。 可以看到里面有一个 cycle 对象，是负责 http 和 asgi 数据转换的对象，它有 send 和 receive 两个方法 1234567891011121314151617181920212223 async def send(self, message: ASGISendEvent) -&gt; None: # 通过传入的参数message获取到ASGI应用程序返回的数据， 并依据ASGI协议进行解析， # 并拼接成HTTP协议的字节流， 当ASGI应用程序发送结束标记时， send会把拼接的字节 # 流通过socket返回给客户端, 同时触发on_response_complete 方法。 ... async def receive(self) -&gt; ASGIReceiveEvent: # 它只负责接收获取到已经解析完成的HTTP数据（前面on_xxx时会把数据传给cycle）， # 然后发送到 ASGI 应用程序中 if self.waiting_for_100_continue and not self.transport.is_closing(): self.transport.write(b&quot;HTTP/1.1 100 Continue\\r \\r &quot;) self.waiting_for_100_continue = False if not self.disconnected and not self.response_complete: self.flow.resume_reading() await self.message_event.wait() self.message_event.clear() if self.disconnected or self.response_complete: return &#123;&quot;type&quot;: &quot;http.disconnect&quot;&#125; message: HTTPRequestEvent = &#123;&quot;type&quot;: &quot;http.request&quot;, &quot;body&quot;: self.body, &quot;more_body&quot;: self.more_body&#125; self.body = b&quot;&quot; return message websocket里面也有两种实现：websockets 和 wsproto websockets 是一个独立的 Python 库，用于构建 WebSocket 服务器和客户端。它提供了一个高级的 API，允许开发者轻松地实现 WebSocket 通信。websockets 库处理了 WebSocket 协议的大部分复杂性，包括握手、帧编码和解码、心跳以及关闭过程。 wsproto 是一个较低级别的 WebSocket 协议库，它提供了 WebSocket 协议的实现，但不包括高级的框架或服务器逻辑。wsproto 主要用于那些需要精细控制 WebSocket 协议细节的场合，或者在性能要求极高的环境中。 流程图 可以看到 http 解析器中只解析到 header，具体的 body 内容是交给 ASGI 应用去处理 supervisorsuvicorn本身是以一个进程启动的， 这个文件夹存放着uvicorn的几种启动方式， 如多进程启动，监控文件变动自动重启的方式等 启动work.py里面有个类 UvicornWorker 用于gunicorn启动 uvicorn 1gunicorn main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:80 logging.py里面定义了一些 formatter 比如根据日志等级渲染不同颜色等 整个项目用到了三个 logger： uvicorn.error 错误日志或生命周期日志 uvicorn.access http 请求访问日志 uvicorn.asgi ASGI 应用相关的日志 日志的定义使用了 dictConfig，之前倒是没用过这种方式配置日志, 感觉不是很方便 1234567891011121314151617181920212223242526class Config: def configure_logging(self) -&gt; None: logging.addLevelName(TRACE_LOG_LEVEL, &quot;TRACE&quot;) if self.log_config is not None: if isinstance(self.log_config, dict): if self.use_colors in (True, False): self.log_config[&quot;formatters&quot;][&quot;default&quot;][&quot;use_colors&quot;] = self.use_colors self.log_config[&quot;formatters&quot;][&quot;access&quot;][&quot;use_colors&quot;] = self.use_colors logging.config.dictConfig(self.log_config) elif self.log_config.endswith(&quot;.json&quot;): with open(self.log_config) as file: loaded_config = json.load(file) logging.config.dictConfig(loaded_config) elif self.log_config.endswith((&quot;.yaml&quot;, &quot;.yml&quot;)): # Install the PyYAML package or the uvicorn[standard] optional # dependencies to enable this functionality. import yaml with open(self.log_config) as file: loaded_config = yaml.safe_load(file) logging.config.dictConfig(loaded_config) else: # See the note about fileConfig() here: # https://docs.python.org/3/library/logging.config.html#configuration-file-format logging.config.fileConfig(self.log_config, disable_existing_loggers=False) main.py123456789101112131415161718192021222324252627def run(app, **kwargs) -&gt; None: if app_dir is not None: sys.path.insert(0, app_dir) # 初始化配置 config = Config(app, **kwargs) # 初始化服务 server = Server(config=config) if (config.reload or config.workers &gt; 1) and not isinstance(app, str): logger = logging.getLogger(&quot;uvicorn.error&quot;) logger.warning(&quot;You must pass the application as an import string to enable &#x27;reload&#x27; or &quot; &quot;&#x27;workers&#x27;.&quot;) sys.exit(1) if config.should_reload: sock = config.bind_socket() ChangeReload(config, target=server.run, sockets=[sock]).run() elif config.workers &gt; 1: sock = config.bind_socket() Multiprocess(config, target=server.run, sockets=[sock]).run() else: server.run() if config.uds and os.path.exists(config.uds): os.remove(config.uds) # pragma: py-win32 if not server.started and not config.should_reload and config.workers == 1: sys.exit(STARTUP_FAILURE) server.py12345678910111213141516171819202122232425262728293031323334class Server: def run(self, sockets: list[socket.socket] | None = None) -&gt; None: # 设置循环策略 self.config.setup_event_loop() # 启动服务 return asyncio.run(self.serve(sockets=sockets)) async def serve(self, sockets: list[socket.socket] | None = None) -&gt; None: with self.capture_signals(): await self._serve(sockets) async def _serve(self, sockets: list[socket.socket] | None = None) -&gt; None: process_id = os.getpid() config = self.config if not config.loaded: config.load() self.lifespan = config.lifespan_class(config) message = &quot;Started server process [%d]&quot; color_message = &quot;Started server process [&quot; + click.style(&quot;%d&quot;, fg=&quot;cyan&quot;) + &quot;]&quot; logger.info(message, process_id, extra=&#123;&quot;color_message&quot;: color_message&#125;) await self.startup(sockets=sockets) if self.should_exit: return await self.main_loop() await self.shutdown(sockets=sockets) message = &quot;Finished server process [%d]&quot; color_message = &quot;Finished server process [&quot; + click.style(&quot;%d&quot;, fg=&quot;cyan&quot;) + &quot;]&quot; logger.info(message, process_id, extra=&#123;&quot;color_message&quot;: color_message&#125;) 我们看看 startup() ，main_loop() 和 shutdown()里面有什么 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162async def startup(self, sockets: list[socket.socket] | None = None) -&gt; None: await self.lifespan.startup() if self.lifespan.should_exit: self.should_exit = True return config = self.config def create_protocol( _loop: asyncio.AbstractEventLoop | None = None, ) -&gt; asyncio.Protocol: return config.http_protocol_class( # type: ignore[call-arg] config=config, server_state=self.server_state, app_state=self.lifespan.state, _loop=_loop, ) loop = asyncio.get_running_loop() listeners: Sequence[socket.SocketType] if sockets is not None: # pragma: full coverage # 当用户传socket过来的时候： 基于该scoket和create_protocol创建服务， # 如果是多进程且是Windows系统， 则要显示的共享socket。 ... elif config.fd is not None: # pragma: py-win32 # 当用户传文件描述符的时候： 基于该文件描述符获取scoket， 并通过该socket # 和create_protocol创建服务。 ... elif config.uds is not None: # pragma: py-win32 # 当用户传unix domain socket的时候: 基于unix domain socket和 # create_protocol创建服务。 ... else: # 当用户传host和port参数的时候: 基于host和port和create_protocol创建服务。 try: server = await loop.create_server( create_protocol, host=config.host, port=config.port, ssl=config.ssl, backlog=config.backlog, ) except OSError as exc: logger.error(exc) await self.lifespan.shutdown() sys.exit(1) assert server.sockets is not None listeners = server.sockets self.servers = [server] if sockets is None: self._log_started_message(listeners) else: # We&#x27;re most likely running multiple workers, so a message has already been # logged by `config.bind_socket()`. pass # pragma: full coverage self.started = True 每次循环执行的时候都会调用on_tick方法， 该方法主要是进行服务统计以及判断啥时候可以退出服务, 比如请求总数超过配置的限制数， 或者收到信号，把变量should_exit设置为True等等 123456789async def main_loop(self) -&gt; None: counter = 0 should_exit = await self.on_tick(counter) # 是个死循环，防止主程序退出 while not should_exit: counter += 1 counter = counter % 864000 await asyncio.sleep(0.1) should_exit = await self.on_tick(counter) 如果在循环中判断程序需要进行退出, 就会进入退出逻辑shutdown 123456789101112131415161718192021222324252627282930313233async def shutdown(self, sockets=None): logger.info(&quot;Shutting down&quot;) # 关闭socket， 不让有新的连接建立 for server in self.servers: server.close() for sock in sockets or []: sock.close() for server in self.servers: await server.wait_closed() # 关闭已经创建的连接， 并等待他们处理完毕 for connection in list(self.server_state.connections): connection.shutdown() await asyncio.sleep(0.1) # 等待连接关闭或者用户强制关闭 if self.server_state.connections and not self.force_exit: msg = &quot;Waiting for connections to close. (CTRL+C to force quit)&quot; logger.info(msg) while self.server_state.connections and not self.force_exit: await asyncio.sleep(0.1) # 等待后台任务完成或者用户强制关闭 if self.server_state.tasks and not self.force_exit: msg = &quot;Waiting for background tasks to complete. (CTRL+C to force quit)&quot; logger.info(msg) while self.server_state.tasks and not self.force_exit: await asyncio.sleep(0.1) # 通过lifespan告诉ASGI应用程序即将关闭 if not self.force_exit: await self.lifespan.shutdown() 启动流程图","tags":["python","uvicron"]},{"title":"Python ORM 集成","path":"/2023/06/25/Python ORM 集成/","content":"python 常用的 ORM 框架非 flask_sqlalchemy 莫属，当然也可以自己直接根据 sqlalchemy 进行封装 flask_sqlalchemy&#x3D;&#x3D;3.0.5 pymysql&#x3D;&#x3D;1.1.0 目录结构 1234567891011121314151617flask_demo:\t-- api: -- file -- file.py -- user -- user.py -- __init__.py\t-- utils -- log.py -- database.py -- restful.py\t-- model -- user.py\t-- service -- user.py\t-- manage.py\t-- requirements.txt 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960utils/database.pyfrom contextlib import contextmanagerfrom flask_sqlalchemy import SQLAlchemyclass MySQLAlchemy(SQLAlchemy): @contextmanager def auto_commit(self): try: yield self.session.commit() except Exception as e: self.session.rollback() raise edb = MySQLAlchemy()model/user.pyfrom model.base import HasTime, AutoincrementId, BaseTablefrom sqlalchemy import Column, String, SMALLINTclass User(BaseTable, HasTime, AutoincrementId): __tablename__ = &quot;user&quot; name = Column(String(10)) email = Column(String(50), nullable=False, unique=True) role = Column(String(10), nullable=False, default=&quot;user&quot;) delete_status = Column(SMALLINT, nullable=False, default=0) service/user.pyfrom model.user import Userclass UserService(object): @staticmethod def query_by_id(user_id: int) -&gt; dict: user = User.query.filter_by(id=user_id).first() return user.to_dict()api/user/user.pyimport loggingfrom flask import Blueprintfrom service.user import UserServicefrom uitls.restful import JsonResfrom uitls.log import log_requestbp = Blueprint(&#x27;user&#x27;, __name__, url_prefix=&quot;/user&quot;)LOG = logging.getLogger(__name__)@bp.route(&#x27;/get&#x27;, methods=[&#x27;GET&#x27;])@log_request()def get_user(): user = UserService.query_by_id(1) return JsonRes(200, True, user)api/__init__.py create_app 方法中添加以下内容 1234567891011sql_config = &#123; &quot;SQLALCHEMY_DATABASE_URI&quot;: &quot;mysql+pymysql://root:root@localhost:3306/flask_demo?charset=utf8&quot;, &quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;: False, &quot;SQLALCHEMY_ECHO&quot;: False, &quot;SQLALCHEMY_POOL_SIZE&quot;: 10, &quot;SQLALCHEMY_POOL_TIMEOUT&quot;: 60, &quot;SQLALCHEMY_POOL_RECYCLE&quot;: 600 &#125; app.config.update(sql_config) db.init_app(app)","tags":["python"]},{"title":"Python flask 框架","path":"/2023/05/22/Python flask 框架/","content":"写一个通用的小型 flask 框架 目录结构 12345678910flask_demo:\t-- api: -- file -- file.py -- __init__.py\t-- utils -- log.py -- restful.py\t-- manage.py\t-- requirements.txt 基准代码api/__init__.py1234567891011121314151617from flask import Flask, Blueprintfrom flask_cors import CORSfrom api.file.file import bp as file_bpdef create_app(): setup_log(log_level=&quot;debug&quot;, log_handler=&quot;file&quot;, log_dir=&quot;./logs&quot;, log_file_name=&quot;app.log&quot;) app = Flask(__name__) CORS(app) app_bp = Blueprint(&#x27;api&#x27;, __name__, url_prefix=&#x27;/api&#x27;) app_bp.register_blueprint(file_bp) app.register_blueprint(app_bp) return app utils/log.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import loggingimport logging.handlersimport os.pathfrom functools import wrapsfrom flask import requestDEBUG_LOG_FORMAT = &#x27;%(threadName)s %(thread)s %(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s &#x27; \\ &#x27;%(funcName)s %(lineno)d [-] %(message)s&#x27;INFO_LOG_FORMAT = &#x27;%(threadName)s %(thread)s %(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s &#x27; \\ &#x27;%(lineno)d [-] %(message)s&#x27;def setup_log(log_level, log_handler, log_dir, log_file_name, backupCount=1): logger = logging.getLogger() if log_level == &quot;DEBUG&quot;: logger.setLevel(logging.DEBUG) formatter = logging.Formatter(DEBUG_LOG_FORMAT) else: logger.setLevel(logging.INFO) formatter = logging.Formatter(INFO_LOG_FORMAT) if log_handler == &quot;file&quot;: if not os.path.exists(log_dir): os.makedirs(log_dir) loghandler = logging.handlers.RotatingFileHandler( filename=os.path.join(log_dir, log_file_name), maxBytes=1000000000, backupCount=backupCount) loghandler.setFormatter(formatter) else: loghandler = logging.StreamHandler() loghandler.setFormatter(formatter) logger.addHandler(loghandler) return loggerdef log_request(f): @wraps(f) def wrapsed(*args, **kwargs): log = logging.getLogger(__name__) log.info(&#x27;API request url %s&#x27;, request.url) if request.query_string: log.info(&#x27;API query string %s&#x27;, request.query_string) log.info(&#x27;API request method %s&#x27;, request.method) if request.method == &quot;POST&quot;: log.info(&quot;API POST data %s&quot;, request.json) if request.method == &quot;PUT&quot;: log.info(&quot;API PUT data %s&quot;, request.json) log.debug(&#x27;API request environ %s&#x27;, request.environ) return f(*args, **kwargs) return wrapsed utils/restful.py1234567891011121314151617181920212223242526272829303132333435import datetimeimport jsonfrom flask import Response_SIMPLE_TYPE = (str, int, type(None), bool, float)def json_encoder(value): if isinstance(value, _SIMPLE_TYPE): return value if isinstance(value, datetime.datetime): return value.isoformat() + &quot;Z&quot; elif isinstance(value, Exception): return &#123; &quot;exception&quot;: value.__class__.__name__, &quot;message&quot;: str(value) &#125;class JsonRes(Response): def __init__(self, code=200, status=True, data=None, error=None): self.res = &#123; &#x27;code&#x27;: code, &#x27;status&#x27;: status, &#125; if data is not None: self.res[&#x27;data&#x27;] = data if error is not None: self.res[&#x27;error&#x27;] = error content = json.dumps(self.res, default=json_encoder) try: super().__init__(content, status=code, mimetype=&quot;application/json&quot;) except TypeError: super(JsonRes, self).__init__(content, status=code, mimetype=&quot;application/json&quot;) manage.py123456from api import create_appapp = create_app()if __name__ == &#x27;__main__&#x27;: app.run(debug=True) file.py123456789101112131415import osfrom flask import Blueprint, requestfrom uitls.api_response import APIResponsefrom uitls.log import log_requestbp = Blueprint(&#x27;file&#x27;, __name__, url_prefix=&quot;/file&quot;)logger = logging.get_logger(__name__)@bp.route(&#x27;/&#x27;, methods=[&#x27;GET&#x27;])@log_requestdef hello(): logger.info(&quot;hello world&quot;) return JsonRes(200, True, &#123;&#x27;hi&#x27;: &#x27;hi&#x27;&#125;)","tags":["python"]},{"title":"Python celery 集成","path":"/2023/05/22/Python celery 集成/","content":"celery&#x3D;&#x3D;5.2.7, redis&#x3D;&#x3D;4.5.5 目录结构 123456789101112flask_demo:\t-- api: -- celery -- celery.py -- file -- file.py -- __init__.py\t-- utils -- log.py -- restful.py\t-- manage.py\t-- requirements.txt 在 api/__init__.py 中添加以下方法 1234567891011121314151617181920212223242526def celery_init_app(app): celery_app = Celery(app.name) celery_app.config_from_object(app.config[&quot;CELERY&quot;]) celery_app.set_default() app.extensions[&quot;celery&quot;] = celery_app return celery_appdef creat_app():\t...some code\tapp = Flask(__name__) app.config.from_mapping( CELERY=dict( broker_url=&quot;redis://localhost:6379/0&quot;, result_backend=&quot;redis://localhost:6379/1&quot;, task_ignore_result=True, task_serializer=&#x27;json&#x27;, accept_content=[&#x27;json&#x27;], result_serializer=&#x27;json&#x27; ), ) app.config.from_prefixed_env() celery_init_app(app) ... another code 在 app.py 12app = create_app()celery = app.extensions[&quot;celery&quot;] 在 utils/tasks.py 中添加任务 不要在 task 方法中调用其他类的方法，task 方法是异步任务，调用其他方法会报错 1234567891011from __future__ import absolute_import, unicode_literalsimport timefrom celery import shared_task@shared_task(ignore_result=False)def test1(x, y): time.sleep(1) return x + y 注意：直接按照 flask 官方文档配置会有错误出现 celery 启动方式 123456789101112131415# windows 只能用 threads 启动，否则任务无法执行celery -A app.celery worker -c 4 -l info -P threads参数说明：\tapp:celery # app 指的是 app.py celery 是 app.py 中的 celery 对象\t-P # 启动方式: prefork：默认的并发方式，即多进程的方式。 eventlet：使用eventlet方式启动worker。 gevent：使用gevent方式启动worker。 solo：单进程的方式。 threads：使用线程的方式\t-l # log 级别\t-c # worker 数量\t当 -P 指定为 gevent 或 eventlet 时，需要安装对应的依赖 celery task 的结果默认存储时间是 1 天，可以通过 result_expires 配置","tags":["python"]},{"title":"Python日志json格式","path":"/2023/05/22/Python日志json格式/","content":"python 在工作中的 log 日志一般是以特殊符号分开的方式，这样的形式方便 debug 查阅，但是并不利于数据处理与分析。json 格式就很不错，可以更好地组织、分析和利用日志数据，从而为应用程序的调试、故障排查和性能优化提供更多的洞察力 示例要求：将 flask 的每个请求都记录下来 log.py 需要安装 pip install python-json-logger 不安装 python-json-logger 也可以实现，只要将 JSON_LOG_FORMATTER 写成 json 字符串的格式，或者使用json.dumps() 你要输出的字段即可，但是这种方式改写的 json 格式 key 和 value 固定为 string 类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import loggingimport osimport timefrom functools import wrapsfrom flask import requestfrom pythonjsonlogger import jsonloggerJSON_LOG_FORMATTER = &quot;%(appName)s %(service)s %(asctime)s %(levelname)s %(message)s %(request)s %(response)s %(rt)s&quot;class JSONFilter(logging.Filter): rt = 0 appName = &quot;myapp&quot; response = &#123;&#125; def request_parse(self): params = &#123;&#125; if request.method == &quot;GET&quot;: params = request.args.to_dict() elif request.method == &quot;POST&quot; or request.method == &quot;PUT&quot;: if request.form: params = request.form.to_dict() else: params = request.get_json() return params def filter(self, record): record.message = record.msg record.service = f&quot;&#123;request.method&#125; &#123;request.path&#125;&quot; record.appName = self.appName record.rt = self.rt record.request = self.request_parse() record.response = self.response return Truedef make_dirs(path): if not os.path.exists(path): os.makedirs(path) return pathdef set_monitor_logger(log_path, name): logger = logging.getLogger(name) logger.setLevel(logging.DEBUG) formatter = jsonlogger.JsonFormatter(JSON_LOG_FORMATTER) json_filter = JSONFilter() logger.addFilter(json_filter) handler = logging.StreamHandler() handler.setFormatter(formatter) file_handler = logging.FileHandler(os.path.join(make_dirs(log_path), f&quot;&#123;name&#125;.log&quot;)) file_handler.setFormatter(formatter) logger.addHandler(handler) logger.addHandler(file_handler) return loggermonitor_logger = set_monitor_logger(&quot;monitor&quot;)def log_request(func): @wraps(func) def wrapped(*args, **kwargs): _filter = monitor_logger.filters[0] start_time = time.time() response = func(*args, **kwargs) _filter.rt = round(time.time() - start_time, 3) _filter.response = response.json monitor_logger.info(&quot;&quot;) return response return wrapped xxxxxxxxxx sql_config &#x3D; { “SQLALCHEMY_DATABASE_URI”: “mysql+pymysql:&#x2F;&#x2F;root:root@localhost:3306&#x2F;flask_demo?charset&#x3D;utf8”, “SQLALCHEMY_TRACK_MODIFICATIONS”: False, “SQLALCHEMY_ECHO”: False, “SQLALCHEMY_POOL_SIZE”: 10, “SQLALCHEMY_POOL_TIMEOUT”: 60, “SQLALCHEMY_POOL_RECYCLE”: 600 } app.config.update(sql_config) db.init_app(app)python","tags":["python"]},{"title":"Python Asyncio","path":"/2023/05/19/Python-Asyncio/","content":"协程 一个抽象概念，在计算机中不存在 协程（Coroutione），也可以被称为微线程，是一种用户态内的上下文切换技术。 实现方法 yield 关键字 asyncio 装饰器（在 py 3.4 之后建议用 第三种方法） async、await 关键字 yield 关键字12345678910111213# 通过 yield 切换上下文def func1(): yield 1 yield from func2() yield 2def func2(): yield 3 yield 4f1 = func1()for item in f1: print(item) asyncio123456789101112131415161718import asyncioasync def func1(): print(1) await asyncio.sleep(2) # 遇到 IO 耗时操作，自动切换到 tasks 中的其他任务 print(2)async def func2(): print(3) await asyncio.sleep(2) print(4)loop = asyncio.get_event_loop()tasks = [asyncio.ensure_future(func1()), asyncio.ensure_future(func2())]loop.run_until_complete(asyncio.wait(tasks))# py 3.7 以后asyncio.run(tasks) 异步编程事件循环理解为一个死循环，去检测并执行某些代码 1234567891011121314# 伪代码task_list = [task1, task2,...]while True: 可执行的任务列表，已完成的任务列表 for 就绪任务 in 可执行任务列表 执行 for 已完成任务 in 已完成任务列表 移除 if 可执行任务为 0： break 协程函数 用 async def func() 定义的函数 12345async def func(): passresult = func()# 协程函数只创建协程对象，函数内部代码不会执行 await await + 可等待对象 （协程对象、Future、IO等待） 等待后面的对象执行完成后在执行下一步任务（单线程内和顺序执行差不多） 123456789101112131415import asyncioasync def func1(): print(1) await asyncio.sleep(2) # 遇到 IO 耗时操作，自动切换到 tasks 中的其他任务 print(2)async def func2(): print(3) await func1() print(4)asyncio.run(func2())# 输出 3，1，2，4 Task 对象 在事件循环中添加多个任务 Tasks 用于并发调度协程，通过 asyncio.create_task() 的方式创建 Task 对象，这样可以让协程加入时间循环中等待被调度执行。除了使用 asyncio.create_task() 函数以外，还可同底层级别的 loop.create_task() 或 asyncio.ensure_future() 函数。不建议手动实例化 Task 对象 注意：asyncio.create_task() 在 python3.7 之后才被加入，该版本之前用 asyncio.ensure_future() 12345678910111213141516171819202122232425import asyncioasync def func(): print(1) await asyncio.sleep(2) print(2) return &quot;the Return&quot;async def main(): print(&quot;main start&quot;) # 创建 task 对象，将 func 添加到事件循环 task1 = asyncio.create_task(func()) task2 = asyncio.create_task(func()) print(&quot;main end&quot;) # 当某协程遇到 阻塞（I/O、sleep）会自动切换到其他任务 ret1 = await task1 ret2 = await task2 print(ret1, ret2)asyncio.run(main()) 用的少，下面的比较常用 123456789101112131415161718192021222324import asyncioasync def func(): print(1) await asyncio.sleep(2) print(2) return &quot;the Return&quot;async def main(): print(&quot;main start&quot;) task_list = [ asyncio.create_task(func()), asyncio.create_task(func()) ] print(&quot;main end&quot;) # done 为执行完之后的结果，pending 为未执行完的结果 done, pending = await asyncio.wait(task_list) print(done, pending)asyncio.run(main()) 第三种 12345678910111213141516import asyncioasync def func(): print(1) await asyncio.sleep(2) print(2) return &quot;the Return&quot;task_list = [ func(), func()]done, pending = asyncio.run(asyncio.wait(task_list))print(done, pending) asyncio.FutureTask 继承 Future 对象，Task 对象内部 await 结果的处理基于 Future 对象来的。 12345678async def main(): # 获取当前事件循环 loop = asyncio.get_running_loop() # # 创建一个任务（Future对象），这个任务什么都不干。 fut = loop.create_future() # 等待任务最终结果（Future对象），没有结果则会一直等下去。 await futasyncio.run(main()) 示例2： 123456789101112131415161718import asyncioasync def set_after(fut): await asyncio.sleep(2) fut.set_result(&quot;666&quot;) async def main(): # 获取当前事件循环 loop = asyncio.get_running_loop() # 创建一个任务（Future对象），没绑定任何行为，则这个任务永远不知道什么时候结束。 fut = loop.create_future() # 创建一个任务（Task对象），绑定了set_after函数，函数内部在2s之后，会给fut赋值。 # 即手动设置future任务的最终结果，那么fut就可以结束了。 await loop.create_task(set_after(fut)) # 等待 Future对象获取 最终结果，否则一直等下去 data = await fut print(data) asyncio.run(main()) Future对象本身函数进行绑定，所以想要让事件循环获取Future的结果，则需要手动设置。而Task对象继承了Future对象，其实就对Future进行扩展，他可以实现在对应绑定的函数执行完成之后，自动执行set_result，从而实现自动结束。 虽然，平时使用的是Task对象，但对于结果的处理本质是基于Future对象来实现的。 concurrent.futures.Future 和 asyncio.Future 没有任何关系 使用线程池、进程池实现异步操作时用到的对象 123456789101112131415import timefrom concurrent.futures import Futurefrom concurrent.futures.thread import ThreadPoolExecutorfrom concurrent.futures.process import ProcessPoolExecutodef func(value): time.sleep(1) print(value) pool = ThreadPoolExecutor(max_workers=5)# 或 pool = ProcessPoolExecutor(max_workers=5)for i in range(10): fut = pool.submit(func, i) print(fut) 两个Future对象是不同的，他们是为不同的应用场景而设计，例如：concurrent.futures.Future不支持await语法 等。 官方提示两对象之间不同： unlike asyncio Futures, concurrent.futures.Future instances cannot be awaited. asyncio.Future.result() and asyncio.Future.exception() do not accept the timeout argument. asyncio.Future.result() and asyncio.Future.exception() raise an InvalidStateError exception when the Future is not done. Callbacks registered with asyncio.Future.add_done_callback() are not called immediately. They are scheduled with loop.call_soon() instead. asyncio Future is not compatible with the concurrent.futures.wait() and concurrent.futures.as_completed() functions. 在Python提供了一个将futures.Future 对象包装成asyncio.Future对象的函数 asynic.wrap_future。 接下里你肯定问：为什么python会提供这种功能？ 其实，一般在程序开发中我们要么统一使用 asycio 的协程实现异步操作、要么都使用进程池和线程池实现异步操作。但如果 协程的异步和 进程池/线程池的异步 混搭时，那么就会用到此功能了。 12345678910111213141516171819202122232425262728import timeimport asyncioimport concurrent.futuresdef func1(): # 某个耗时操作 time.sleep(2) return &quot;SB&quot;async def main(): loop = asyncio.get_running_loop() # 1. Run in the default loop&#x27;s executor ( 默认ThreadPoolExecutor ) # 第一步：内部会先调用 ThreadPoolExecutor 的 submit 方法去线程池中申请一个线程去执行func1函数，并返回一个concurrent.futures.Future对象 # 第二步：调用asyncio.wrap_future将concurrent.futures.Future对象包装为asycio.Future对象。 # 因为concurrent.futures.Future对象不支持await语法，所以需要包装为 asycio.Future对象 才能使用。 fut = loop.run_in_executor(None, func1) result = await fut print(&#x27;default thread pool&#x27;, result) # 2. Run in a custom thread pool: # with concurrent.futures.ThreadPoolExecutor() as pool: # result = await loop.run_in_executor( # pool, func1) # print(&#x27;custom thread pool&#x27;, result) # 3. Run in a custom process pool: # with concurrent.futures.ProcessPoolExecutor() as pool: # result = await loop.run_in_executor( # pool, func1) # print(&#x27;custom process pool&#x27;, result)asyncio.run(main()) 应用场景：当项目以协程式的异步编程开发时，如果要使用一个第三方模块，而第三方模块不支持协程方式异步编程时，就需要用到这个功能，例如： 12345678910111213141516171819202122232425import asyncioimport requestsasync def download_image(url): # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动化切换到其他任务） print(&quot;开始下载:&quot;, url) loop = asyncio.get_event_loop() # requests模块默认不支持异步操作，所以就使用线程池来配合实现了。 future = loop.run_in_executor(None, requests.get, url) response = await future print(&#x27;下载完成&#x27;) # 图片保存到本地文件 file_name = url.rsplit(&#x27;_&#x27;)[-1] with open(file_name, mode=&#x27;wb&#x27;) as file_object: file_object.write(response.content) if __name__ == &#x27;__main__&#x27;: url_list = [ &#x27;https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg&#x27;, &#x27;https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg&#x27;, &#x27;https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg&#x27; ] tasks = [download_image(url) for url in url_list] loop = asyncio.get_event_loop() loop.run_until_complete( asyncio.wait(tasks) ) 异步迭代器uvloopPython标准库中提供了asyncio模块，用于支持基于协程的异步编程。 uvloop是 asyncio 中的事件循环的替代方案，替换后可以使得asyncio性能提高。事实上，uvloop要比nodejs、gevent等其他python异步框架至少要快2倍，性能可以比肩Go语言。 安装uvloop 1pip3 install uvloop 在项目中想要使用uvloop替换asyncio的事件循环也非常简单，只要在代码中这么做就行。 12345678import asyncioimport uvloopasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())# 编写asyncio的代码，与之前写的代码一致。# 内部的事件循环自动化会变为uvloopasyncio.run(...) 注意：知名的asgi uvicorn内部就是使用的uvloop的事件循环。 实战案例异步 Redis当通过 Python 去操作 redis 时，连接、设置值、获取值 这些都涉及网络 IO 请求，使用 asyncio 异步的方式可以在 IO 等待时去做一些其他任务，从而提升性能 安装Python异步操作redis模块 1pip3 install aioredis 示例1：异步操作redis。 1234567891011121314151617181920#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport aioredisasync def execute(address, password): print(&quot;开始执行&quot;, address) # 网络IO操作：创建redis连接 redis = await aioredis.create_redis(address, password=password) # 网络IO操作：在redis中设置哈希值car，内部在设三个键值对，即： redis = &#123; car:&#123;key1:1,key2:2,key3:3&#125;&#125; await redis.hmset_dict(&#x27;car&#x27;, key1=1, key2=2, key3=3) # 网络IO操作：去redis中获取值 result = await redis.hgetall(&#x27;car&#x27;, encoding=&#x27;utf-8&#x27;) print(result) redis.close() # 网络IO操作：关闭redis连接 await redis.wait_closed() print(&quot;结束&quot;, address) asyncio.run(execute(&#x27;redis://47.93.4.198:6379&#x27;, &quot;root!2345&quot;)) 示例2：连接多个redis做操作（遇到IO会切换其他任务，提供了性能）。 1234567891011121314151617181920212223import asyncioimport aioredisasync def execute(address, password): print(&quot;开始执行&quot;, address) # 网络IO操作：先去连接 47.93.4.197:6379，遇到IO则自动切换任务，去连接47.93.4.198:6379 redis = await aioredis.create_redis_pool(address, password=password) # 网络IO操作：遇到IO会自动切换任务 await redis.hmset_dict(&#x27;car&#x27;, key1=1, key2=2, key3=3) # 网络IO操作：遇到IO会自动切换任务 result = await redis.hgetall(&#x27;car&#x27;, encoding=&#x27;utf-8&#x27;) print(result) redis.close() # 网络IO操作：遇到IO会自动切换任务 await redis.wait_closed() print(&quot;结束&quot;, address) task_list = [ execute(&#x27;redis://47.93.4.197:6379&#x27;, &quot;root!2345&quot;), execute(&#x27;redis://47.93.4.198:6379&#x27;, &quot;root!2345&quot;)]asyncio.run(asyncio.wait(task_list)) 官网：https://aioredis.readthedocs.io/en/v1.3.0/start.html 爬虫123456789101112131415161718192021import aiohttpimport asyncioasync def fetch(session, url): print(&quot;发送请求：&quot;, url) async with session.get(url, verify_ssl=False) as response: text = await response.text() print(&quot;得到结果：&quot;, url, len(text)) async def main(): async with aiohttp.ClientSession() as session: url_list = [ &#x27;https://python.org&#x27;, &#x27;https://www.baidu.com&#x27;, &#x27;https://www.pythonav.com&#x27; ] tasks = [asyncio.create_task(fetch(session, url)) for url in url_list] await asyncio.wait(tasks) if __name__ == &#x27;__main__&#x27;: asyncio.run(main()) 对比requests requests.post 每次都会创建新的连接，速度较慢。如果首先初始化一个 session，那么 requests 会保持连接，大大提高请求速度 12session = requests.Session()# session 只对同一个链接请求多次的场景下有效 不借助其他第三方库的情况下 requests：只能发送同步请求 aiohttp: 只能发送异步请求 httpx: 既能发送同步请求，又能发送异步请求 总结 如果你只发几条请求。那么使用 requests 或者 httpx 的同步模式，代码最简单。 requests 是否创建一个 session 保持连接，速度差别比较大，在没有反爬的情况下，只追求速度，建议用 requests.session () 如果你要发送很多请求，但是有些地方要发送同步请求，有些地方要发送异步请求，那么使用 httpx 最省事。 如果你要发送很多请求，并且越快越好，那么使用 aiohttp 最快。 问题路径1234os.path.join(cur_path, &quot;\\asdff.csv&quot;)字符串 &quot;\\asdff.csv&quot; 会导致从系统根目录开始加载当 flask 运行时，os.getcwd() 获取的时项目根目录","tags":["python"]},{"title":"Python 文件上传","path":"/2023/05/19/Python-文件上传/","content":"针对大文件分片上传然后合并可以使用前端解决方案 webuploader 组件 后端接口1234567891011121314151617181920212223242526272829303132333435363738394041import osfrom flask import Blueprint, requestfrom uitls.api_response import JsonResfrom uitls.log import log_requestbp = Blueprint(&#x27;file&#x27;, __name__, url_prefix=&quot;/file&quot;)@bp.route(&#x27;/upload/accept&#x27;, methods=[&#x27;POST&#x27;])@log_request()def upload(): upload_file = request.files[&#x27;file&#x27;] task = request.form.get(&#x27;task_id&#x27;) chunk = request.form.get(&#x27;chunk&#x27;, 0) filename = &#x27;%s%s&#x27; % (task, chunk) upload_file.save(&#x27;./upload/%s&#x27; % filename) return JsonRes(200, True, &#123;&#x27;filename&#x27;: filename&#125;)@bp.route(&quot;/upload/complete&quot;, methods=[&#x27;GET&#x27;])@log_request()def upload_complete(): target_filename = request.args.get(&#x27;filename&#x27;) task = request.args.get(&#x27;task_id&#x27;) chunk = 0 with open(&#x27;./upload/%s&#x27; % target_filename, &#x27;wb&#x27;) as target_file: while True: try: filename = &#x27;./upload/%s%d&#x27; % (task, chunk) source_file = open(filename, &#x27;rb&#x27;) target_file.write(source_file.read()) source_file.close() except IOError: break chunk += 1 os.remove(filename) return JsonRes(200, True) 前端页面12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970index.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;script src=&quot;../static/jquery-1.11.1.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;../static/bootstrap/js/bootstrap.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;../static/webuploader/webuploader.min.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../static/webuploader/webuploader.css&quot;&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../static/bootstrap/css/bootstrap.min.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;div id=&quot;picker&quot;&gt;请选择&lt;/div&gt; &lt;!-- 上传按钮，必须指定id选择器的值 --&gt; &lt;div class=&quot;progress&quot;&gt; &lt;!-- 进度条 --&gt; &lt;div class=&quot;progress-bar progress-bar-striped active&quot; role=&quot;progressbar&quot; style=&quot;width:0%;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(document).ready(function() &#123; var task_id = WebUploader.Base.guid(); //产生task_id var uploader = WebUploader.create(&#123; //创建上传控件 swf: &#x27;../static/webuploader/Uploader.swf&#x27;, //swf位置，这个可能与flash有关 server: &#x27;http://localhost:5000/api/file/upload/accept&#x27;, //接收每一个分片的服务器地址 pick: &#x27;#picker&#x27;, //填上传按钮的id选择器值 auto: true, //选择文件后，是否自动上传 chunked: true, //是否分片 chunkSize: 20 * 1024 * 1024, //每个分片的大小，这里为20M chunkRetry: 3, //某分片若上传失败，重试次数 threads: 1, //线程数量，考虑到服务器，这里就选了1 duplicate: true, //分片是否自动去重 formData: &#123; //每次上传分片，一起携带的数据 task_id: task_id, &#125;, &#125;); uploader.on(&#x27;startUpload&#x27;, function() &#123; //开始上传时，调用该方法 $(&#x27;.progress-bar&#x27;).css(&#x27;width&#x27;, &#x27;0%&#x27;); $(&#x27;.progress-bar&#x27;).text(&#x27;0%&#x27;); &#125;); uploader.on(&#x27;uploadProgress&#x27;, function(file, percentage) &#123; //一个分片上传成功后，调用该方法 $(&#x27;.progress-bar&#x27;).css(&#x27;width&#x27;, percentage * 100 - 1 + &#x27;%&#x27;); $(&#x27;.progress-bar&#x27;).text(Math.floor(percentage * 100 - 1) + &#x27;%&#x27;); &#125;); uploader.on(&#x27;uploadSuccess&#x27;, function(file) &#123; //整个文件的所有分片都上传成功，调用该方法 //上传的信息（文件唯一标识符，文件名） var data = &#123;&#x27;task_id&#x27;: task_id, &#x27;filename&#x27;: file.source[&#x27;name&#x27;] &#125;; $.get(&#x27;http://localhost:5000/api/file/upload/complete&#x27;, data); //ajax携带data向该url发请求 $(&#x27;.progress-bar&#x27;).css(&#x27;width&#x27;, &#x27;100%&#x27;); $(&#x27;.progress-bar&#x27;).text(&#x27;上传完成&#x27;); &#125;); uploader.on(&#x27;uploadError&#x27;, function(file) &#123; //上传过程中发生异常，调用该方法 $(&#x27;.progress-bar&#x27;).css(&#x27;width&#x27;, &#x27;100%&#x27;); $(&#x27;.progress-bar&#x27;).text(&#x27;上传失败&#x27;); &#125;); uploader.on(&#x27;uploadComplete&#x27;, function(file) &#123;//上传结束，无论文件最终是否上传成功，该方法都会被调用 $(&#x27;.progress-bar&#x27;).removeClass(&#x27;active progress-bar-striped&#x27;); &#125;); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 前端页面依赖的 js css 等可以在 https://github.com/xinmos/xinmos.github.io/tree/master/asset/upload/ 下载","tags":["python"]},{"title":"Python 装饰器","path":"/2023/05/19/Python-装饰器/","content":"在 python flask 一文中 utils/log.py 中定义了一个装饰器，用于将所有的网络请求记入到日志中 那如何定义一个带参数的装饰器呢？ 最简单的装饰器123456789101112131415161718192021222324252627282930313233343536373839from functools import wrapsdef decorator(func): def wrapper(*args, **kwargs): # 添加额外的功能或修改行为 print(&quot;在调用函数之前做一些事情&quot;) result = func(*args, **kwargs) print(&quot;在调用函数之后做一些事情&quot;) return result return wrapperdef decorator2(func): @wraps(func) def wrapper(*args, **kwargs): # 添加额外的功能或修改行为 print(&quot;在调用函数之前做一些事情&quot;) result = func(*args, **kwargs) print(&quot;在调用函数之后做一些事情&quot;) return result return wrapper@decoratordef test1(): print(&quot;run test1&quot;)@decorator2def test2(): print(&quot;run test2&quot;)if __name__ == &#x27;__main__&#x27;: test1() print(&quot;test1 func name: &quot;, test1.__name__) test2() print(&quot;test2 func name: &quot;, test2.__name__) 输出: 12345678在调用函数之前做一些事情run test1在调用函数之后做一些事情test1 func name: wrapper在调用函数之前做一些事情run test2在调用函数之后做一些事情test2 func name: test2 @wraps 作用@wraps是Python中的一个装饰器，它可以用来将被装饰函数的元信息（如函数名、参数列表等）复制到装饰器函数中，从而使得装饰器函数也具有被装饰函数的元信息。这样做的好处是，可以让被装饰函数在使用时更加方便，因为它们的元信息不会被修改 带参数的装饰器在装饰器的外面再包裹一层 1234567891011121314151617181920212223from functools import wrapsdef log_request(on=True): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(&quot;args: on=&quot;, on) print(&quot;在调用函数之前做一些事情&quot;) result = func(*args, **kwargs) print(&quot;在调用函数之后做一些事情&quot;) return result return wrapper return decorator@log_request(on=False)def test(): print(&quot;run test&quot;)if __name__ == &#x27;__main__&#x27;: test() 输出： 1234args: on= False在调用函数之前做一些事情run test在调用函数之后做一些事情 可以使用 @log_request() 但是无法直接使用 @log_request，即使 on 已经设置了默认值。 兼容型装饰器12345678910111213141516171819202122232425262728293031323334353637383940from functools import wrapsdef log_request(origin_func=None, on=True): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(&quot;args: on=&quot;, on) print(&quot;在调用函数之前做一些事情&quot;) result = func(*args, **kwargs) print(&quot;在调用函数之后做一些事情&quot;) return result return wrapper if origin_func is None: return decorator else: return decorator(origin_func)@log_requestdef test(): print(&quot;run test&quot;)@log_request()def test2(): print(&quot;run test2&quot;)@log_request(on=False)def test3(): print(&quot;run test3&quot;)if __name__ == &#x27;__main__&#x27;: test() print(&quot;-------------&quot;) test2() print(&quot;-------------&quot;) test3() 输出： 1234567891011121314args: on= True在调用函数之前做一些事情run test在调用函数之后做一些事情-------------args: on= True在调用函数之前做一些事情run test2在调用函数之后做一些事情-------------args: on= False在调用函数之前做一些事情run test3在调用函数之后做一些事情","tags":["python"]}]